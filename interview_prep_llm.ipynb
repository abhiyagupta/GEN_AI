{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPSBVCRGSxS1Ky6w+LuUsMT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhiyagupta/GEN_AI/blob/main/interview_prep_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Dtlbvx4GMkpw",
        "outputId": "666314ce-645f-4eb2-a06f-d9525e135a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.0/384.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.7/360.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.2/140.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain-openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract --quiet\n",
        "!pip install Pillow   -- quiet\n",
        "!pip install langchain_community --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc-OI8XqMqOv",
        "outputId": "8b3f45d3-9dbb-4088-e126-87ac41ad4cfd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement quiet (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for quiet\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.6/990.6 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "38hAcH8sNQZu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "wXXRCTbINbfh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain"
      ],
      "metadata": {
        "id": "Qm280qu2Nr-1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain"
      ],
      "metadata": {
        "id": "Hd6lyVGaNmeh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "XEGqbA_sN-EE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "S9gpfi1fUeug"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "cw-redCFUspt"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document=\"I have an interview tomorrow. Topic is NLP and LLM\""
      ],
      "metadata": {
        "id": "aliPksW8OsHn"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "wbuPcXF1US88"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
      ],
      "metadata": {
        "id": "PLGBvHjzVXm1"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template1= \"\"\"\n",
        "              take the {input_data} provided and do the following:\n",
        "              1. read the {input_data}\n",
        "              2. give ONLY FIVE QUESTIONS based on interview type for preparation.\n",
        "              3. do not give any answer.\n",
        "              3. provide question realted to only topic mentioned in {input_data}\n",
        "              4. do not make up own topics.\n",
        "\n",
        "              data is given here: {input_data}\n",
        "\n",
        "          \"\"\""
      ],
      "metadata": {
        "id": "lpPvVhtuVhON"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = PromptTemplate(\n",
        "           input_varibales=[\"input_data\"],\n",
        "           template=template1\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "lksS7lc1V_X6"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain1 = LLMChain(\n",
        "              llm=llm,\n",
        "              prompt=prompt1,\n",
        "              output_key=\"questions\"\n",
        ")"
      ],
      "metadata": {
        "id": "46XCEYIWWPzR"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
        "template2=\"\"\"\n",
        "            read the data in {input_data} and answer the question given in {input_data}:\n",
        "            1. answer should be simple and consice for job interview.\n",
        "            2. if possible include example\n",
        "            3. give both the question generated followed by answer.\n",
        "            4. wrap the question and answer such that it is easy to read\n",
        "          \"\"\"\n",
        "prompt2 = PromptTemplate(\n",
        "           input_varibales=[\"questions\"],\n",
        "           template=template2\n",
        ")\n",
        "chain2 = LLMChain(\n",
        "              llm=llm,\n",
        "              prompt=prompt2,\n",
        "              output_key=\"answer\"\n",
        ")"
      ],
      "metadata": {
        "id": "3wvEBJfeXLEF"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain"
      ],
      "metadata": {
        "id": "1dmzhqeLZ962"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain=SequentialChain(\n",
        "            chains=[chain1,chain2],\n",
        "            input_variables=[\"input_data\"],\n",
        "            output_variables=[\"questions\",\"answer\"]\n",
        ")\n",
        "\n",
        "result=full_chain.invoke({\"input_data\":document})"
      ],
      "metadata": {
        "id": "R_YoMMdqcvV_"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display, HTML"
      ],
      "metadata": {
        "id": "5ctlkXoEhk5O"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(\"<style>.output_area pre { white-space: pre-wrap; }</style>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "iw7YZTkCiJ3B",
        "outputId": "e21f6c1f-b9f2-4c25-e667-def5d4400823"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.output_area pre { white-space: pre-wrap; }</style>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlI19oVrhPzb",
        "outputId": "b40cba47-c405-4fce-b0c3-b92545a979a5"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here are some potential questions and answers for your interview on NLP (Natural Language Processing) and LLM (Large Language Models):\n",
            "\n",
            "---\n",
            "\n",
            "**Q1: What is Natural Language Processing (NLP)?**\n",
            "\n",
            "**A1:** NLP is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. It involves enabling computers to understand, interpret, and generate human language.\n",
            "\n",
            "*Example:* Applications of NLP include chatbots, sentiment analysis, and language translation.\n",
            "\n",
            "---\n",
            "\n",
            "**Q2: What are Large Language Models (LLMs)?**\n",
            "\n",
            "**A2:** LLMs are a type of artificial intelligence model that are trained on vast amounts of text data to understand and generate human language. They use deep learning techniques to predict and generate text based on the input they receive.\n",
            "\n",
            "*Example:* GPT-3 by OpenAI is an example of an LLM that can generate human-like text based on prompts.\n",
            "\n",
            "---\n",
            "\n",
            "**Q3: How do LLMs differ from traditional NLP models?**\n",
            "\n",
            "**A3:** LLMs are trained on much larger datasets and have significantly more parameters than traditional NLP models. This allows them to generate more coherent and contextually relevant text. Traditional NLP models often require more task-specific training and feature engineering.\n",
            "\n",
            "*Example:* Traditional models might need separate training for tasks like translation and summarization, while an LLM can handle both with minimal additional training.\n",
            "\n",
            "---\n",
            "\n",
            "**Q4: What is tokenization in NLP?**\n",
            "\n",
            "**A4:** Tokenization is the process of breaking down text into smaller units called tokens, which can be words, subwords, or characters. This is a crucial step in preparing text data for further processing by NLP models.\n",
            "\n",
            "*Example:* The sentence \"I love NLP\" can be tokenized into [\"I\", \"love\", \"NLP\"].\n",
            "\n",
            "---\n",
            "\n",
            "**Q5: Can you explain the concept of attention mechanism in LLMs?**\n",
            "\n",
            "**A5:** The attention mechanism allows LLMs to focus on different parts of the input text when generating output. It helps the model weigh the importance of different words in a sentence, improving the quality of tasks like translation and summarization.\n",
            "\n",
            "*Example:* In translating \"The cat sat on the mat\" to another language, the attention mechanism helps the model understand the relationship between \"cat\" and \"sat\" to produce a coherent translation.\n",
            "\n",
            "---\n",
            "\n",
            "**Q6: What are some common applications of NLP and LLMs?**\n",
            "\n",
            "**A6:** Common applications include machine translation, sentiment analysis, chatbots, text summarization, and question-answering systems.\n",
            "\n",
            "*Example:* Google Translate uses NLP and LLMs to provide real-time language translation.\n",
            "\n",
            "---\n",
            "\n",
            "**Q7: What are the challenges in NLP and LLMs?**\n",
            "\n",
            "**A7:** Challenges include handling ambiguity in language, understanding context, managing large datasets, and ensuring ethical use of AI.\n",
            "\n",
            "*Example:* Sarcasm detection is difficult because it requires understanding context and tone, which are challenging for models to interpret accurately.\n",
            "\n",
            "---\n",
            "\n",
            "I hope these questions and answers help you prepare for your interview. Good luck!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OlofgLdJhSs5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}