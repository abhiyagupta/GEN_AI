{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using ChatGPT or Gemini with Python and LangChain for real-world tasks\n",
        "\n",
        "In this notebook you will use ChatGPT or Gemini and LangChain to solve and learn about:\n",
        "\n",
        "- Langchain chains\n",
        "- Memory and conversation chains\n",
        "\n",
        "- Exercise 1: Review Analysis and Response\n",
        "- Exercise 2: Paper Analysis and Summarization\n",
        "\n",
        "\n",
        "- Bonus: Build a text-based chatbot\n",
        "\n",
        "___[Created By: Dipanjan (DJ)](https://www.linkedin.com/in/dipanjans/)___"
      ],
      "metadata": {
        "id": "XTzBUFWQ-OWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI and LangChain dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2evPp14fy258",
        "outputId": "4878ee47-e0f9-487b-cb8b-644b714def72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-openai\n",
            "  Downloading langchain_openai-0.1.3-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
            "  Downloading langchain_community-0.0.32-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
            "  Downloading langchain_core-0.1.42-py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.45-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.2/104.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Collecting openai<2.0.0,>=1.10.0 (from langchain-openai)\n",
            "  Downloading openai-1.17.0-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.3/268.3 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.5.2 (from langchain-openai)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.10.0->langchain-openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.2.0)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, h11, typing-inspect, tiktoken, marshmallow, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, openai, langchain-core, langchain-text-splitters, langchain-openai, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.32 langchain-core-0.1.42 langchain-openai-0.1.3 langchain-text-splitters-0.0.1 langsmith-0.1.45 marshmallow-3.21.1 mypy-extensions-1.0.0 openai-1.17.0 orjson-3.10.0 packaging-23.2 tiktoken-0.6.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional: Install LangChain Google Gemini Dependency\n",
        "\n",
        "Google Gemini API is free (till now). You can get a key [here](https://aistudio.google.com/app/apikey), just need to sign in with your google account. Gemini may not be available fully in EU."
      ],
      "metadata": {
        "id": "s3XbTsMsuEn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s7rII_GgugOW",
        "outputId": "962af5cc-9324-4753-854f-c48d613d2187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-1.0.2-py3-none-any.whl (28 kB)\n",
            "Collecting google-generativeai<0.6.0,>=0.5.0 (from langchain-google-genai)\n",
            "  Downloading google_generativeai-0.5.0-py3-none-any.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.2,>=0.1.27 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.1.42)\n",
            "Collecting google-ai-generativelanguage==0.6.1 (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.1-py3-none-any.whl (663 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.6/663.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (2.11.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (2.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (4.11.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.1->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (1.23.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.27->langchain-google-genai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.27->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.27->langchain-google-genai) (0.1.45)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.27->langchain-google-genai) (23.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.27->langchain-google-genai) (8.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.27->langchain-google-genai) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1.27->langchain-google-genai) (3.10.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1.27->langchain-google-genai) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (2.16.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (1.63.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.1.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (1.62.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1.27->langchain-google-genai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1.27->langchain-google-genai) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1.27->langchain-google-genai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1.27->langchain-google-genai) (2024.2.2)\n",
            "Installing collected packages: google-ai-generativelanguage, google-generativeai, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.4.0\n",
            "    Uninstalling google-ai-generativelanguage-0.4.0:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.4.0\n",
            "  Attempting uninstall: google-generativeai\n",
            "    Found existing installation: google-generativeai 0.3.2\n",
            "    Uninstalling google-generativeai-0.3.2:\n",
            "      Successfully uninstalled google-generativeai-0.3.2\n",
            "Successfully installed google-ai-generativelanguage-0.6.1 google-generativeai-0.5.0 langchain-google-genai-1.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "5917ee6c843c495c919edad086a8a0a5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load OpenAI API Credentials\n",
        "\n",
        "Here we load it from a file so we don't explore the credentials on the internet by mistake"
      ],
      "metadata": {
        "id": "CiwGjVWK4q6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "5e1HqI56y7t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryheOZuXxa41"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "with open('chatgpt_api_credentials.yml', 'r') as file:\n",
        "    api_creds = yaml.safe_load(file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_creds.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZs7ts6NzADJ",
        "outputId": "eca228d8-420a-439b-a3e7-b0d97a13aca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['openai_key'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = api_creds['openai_key']"
      ],
      "metadata": {
        "id": "kDe44J0N0NcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Load Gemini API credentials\n",
        "\n",
        "Run this section only if you are using Google Gemini"
      ],
      "metadata": {
        "id": "LS7koM2emZ_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "\n",
        "with open('gemini_key.yml', 'r') as file:\n",
        "    api_creds = yaml.safe_load(file)\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = api_creds['gemini_key']"
      ],
      "metadata": {
        "id": "nxJAcO1MmhRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Necessary Dependencies and ChatGPT LLM"
      ],
      "metadata": {
        "id": "VDWhgxCy5bA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "9GYhyRFRuJXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.0)"
      ],
      "metadata": {
        "id": "mY2bapqfuWq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Load Google Gemini LLM\n",
        "\n",
        "Only run the below cell if you don't want to use ChatGPT and want to use Google Gemini"
      ],
      "metadata": {
        "id": "6CHIZflB3X6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini_model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
        "                                      convert_system_message_to_human=True)"
      ],
      "metadata": {
        "id": "YGXBXnHd3g6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's look at how to create a basic chain"
      ],
      "metadata": {
        "id": "qJrIiENwxMx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \"tell me a joke about {topic}\"\n",
        "prompt = ChatPromptTemplate.from_template(PROMPT)\n",
        "\n",
        "chain = (\n",
        "         prompt\n",
        "         |\n",
        "         model\n",
        ")\n",
        "\n",
        "response = chain.invoke({\"topic\": \"bears\"})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Evdag0KuOXo",
        "outputId": "c7b8c071-8c7f-4c40-8842-4f0b9a571b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the bear break up with his girlfriend? \n",
            "\n",
            "Because he couldn't bear the relationship any longer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same code with gemini\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
        "chain = (\n",
        "         prompt\n",
        "         |\n",
        "         gemini_model\n",
        ")\n",
        "\n",
        "response = chain.invoke({\"topic\": \"bears\"})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN-2Plhs3oTX",
        "outputId": "e8134e85-97e8-4a18-868f-bd55bb5acf38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:308: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What do you call a bear with no teeth?\n",
            "\n",
            "A gummy bear.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: Replace model with gemini_model in any of the code below if you want to use Google Gemini instead of ChatGPT"
      ],
      "metadata": {
        "id": "na0SulKs4hXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# can be used on multiple prompts also\n",
        "topics = [{'topic': 'AI'}, {'topic': 'Statistics'}]\n",
        "responses = chain.map().invoke(topics)"
      ],
      "metadata": {
        "id": "Tp84pgKXvKuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2413f66d-cdc4-4699-b5d1-27ba2b75f7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:308: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:308: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for response in responses:\n",
        "  print(response.content)\n",
        "  print('-----')\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QePjecVqviA3",
        "outputId": "4d5f2588-7c5d-44e4-9ab9-26be9bec2330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the AI cross the road?\n",
            "\n",
            "To get to the other algorithm.\n",
            "-----\n",
            "\n",
            "\n",
            "Why did the statistician get lost?\n",
            "\n",
            "Because he didn't take the mean.\n",
            "-----\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic chains are ad-hoc - No conversation history!"
      ],
      "metadata": {
        "id": "jLH8WUaumNMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"{query}\")\n",
        "basic_chain = (\n",
        "               prompt\n",
        "               |\n",
        "              model\n",
        ")"
      ],
      "metadata": {
        "id": "_LZSwWNCx2EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = basic_chain.invoke({\"query\" : 'What are the first four colors of the rainbow?'})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhvUjj6Vx8QV",
        "outputId": "3710cfc7-88cd-49d4-e783-e6a4dfada482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first four colors of the rainbow are red, orange, yellow, and green.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = basic_chain.invoke({\"query\" : 'And the other three?'})\n",
        "print(response.content) # gives a totally random response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9nLc6PnySWh",
        "outputId": "94f80c37-f6d6-421e-d4cd-8f6c675aa22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The other three are: \n",
            "\n",
            "- The Father\n",
            "- The Son\n",
            "- The Holy Spirit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's learn how to add memory to build a conversation chain"
      ],
      "metadata": {
        "id": "Y6NeOZe6mZ8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
      ],
      "metadata": {
        "id": "AEQ7Ame0yWwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create prompt template\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Act as a helpful AI Assistant\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)"
      ],
      "metadata": {
        "id": "yQliFt8lzdRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({}) # shows the conversation history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihya5We_RPdH",
        "outputId": "fb49464a-e2c0-406c-924b-fc885722687b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': []}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "# creates the conversation chain\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(memory.load_memory_variables)\n",
        "        |\n",
        "        itemgetter(\"history\")\n",
        "    )\n",
        "    |\n",
        "    prompt\n",
        "    |\n",
        "    model\n",
        ")"
      ],
      "metadata": {
        "id": "66AQdRNaRYYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = {'input': 'What are the first four colors of a rainbow'}\n",
        "response = chain.invoke(user_input)\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wfgXhIU-U05R",
        "outputId": "99501de8-4ac7-4707-c464-10ac40812878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The first four colors of a rainbow are red, orange, yellow, and green.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVepjfOZMf_l",
        "outputId": "a8877725-984d-4cd8-c6e7-cc0250a035a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': []}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNqE-CTfnQ-4",
        "outputId": "f060b7f0-416c-4034-e352-e633d813f670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What are the first four colors of a rainbow'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\"output\": response.content}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIwgn8UbnTYU",
        "outputId": "d0ecc2a6-a42d-4a52-8dc4-58979d66e1cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': 'The first four colors of a rainbow are red, orange, yellow, and green.'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context(user_input, {\"output\": response.content})\n",
        "memory.load_memory_variables({}) # remembers the conversation history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83bpMrWKVG_H",
        "outputId": "a0aef18e-d2a0-40a5-febf-ae0115fff406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='What are the first four colors of a rainbow'),\n",
              "  AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = {'input': 'And the last 3?'}\n",
        "response = chain.invoke(user_input) # uses history of the past conversation to give a better response\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7w2ENCJJVjHs",
        "outputId": "6aef02ef-b936-408c-fe53-f13981b44639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The last three colors of a rainbow are blue, indigo, and violet.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context(user_input, {\"output\": response.content})\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzGwOdPuVfwy",
        "outputId": "8052bc4b-6cf5-4666-8cb4-930ea91ddf44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='What are the first four colors of a rainbow'),\n",
              "  AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.'),\n",
              "  HumanMessage(content='And the last 3?'),\n",
              "  AIMessage(content='The last three colors of a rainbow are blue, indigo, and violet.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Review Analysis and Response\n",
        "\n",
        "For each review get ChatGPT to do the following and use regular llm chains:\n",
        "\n",
        "  - Summarize the review below, delimited by triple\n",
        "  backticks. The summary should be at most 3 lines.\n",
        "  - Highlight both the positives and negatives\n",
        "  - Display the overall sentiment of the review (positive, negative, neutral)\n",
        "  - Display a list of 3 - 5 emotions expressed by the customer in the review\n",
        "  - If the sentiment is positive or neutral write an email and thank them for the review\n",
        "  - If the sentiment is negative apologize and write an email with an appropriate response\n",
        "\n",
        "\n",
        "Remember you can pass the list of documents using the `map().invoke(...)` function with your `chain`"
      ],
      "metadata": {
        "id": "AeDkpvGDhMGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = [\n",
        "    f\"\"\"\n",
        "    Just received the Bluetooth speaker I ordered for beach outings, and it's fantastic.\n",
        "    The sound quality is impressively clear with just the right amount of bass.\n",
        "    It's also waterproof, which tested true during a recent splashing incident.\n",
        "    Though it's compact, the volume can really fill the space.\n",
        "    The price was a bargain for such high-quality sound.\n",
        "    Shipping was also on point, arriving two days early in secure packaging.\n",
        "    \"\"\",\n",
        "    f\"\"\"\n",
        "    Purchased a new gaming keyboard because of its rave reviews about responsiveness and backlighting.\n",
        "    It hasn't disappointed. The keys have a satisfying click and the LED colors are vibrant,\n",
        "    enhancing my gaming experience significantly. Price-wise, it's quite competitive,\n",
        "    and I feel like I got a good deal. The delivery was swift, and it came well-protected,\n",
        "    ensuring no damage during transport.\n",
        "    \"\"\",\n",
        "    f\"\"\"\n",
        "    Ordered a set of wireless earbuds for running, and they've been a letdown.\n",
        "    The sound constantly cuts out, and the fit is uncomfortable after only a few minutes of use.\n",
        "    They advertised a 12-hour battery life, but I'm barely getting four hours.\n",
        "    Considering the cost, I expected better quality and performance.\n",
        "    They did arrive on time, but the positives end there. I'm already looking into a return.\n",
        "    \"\"\",\n",
        "    f\"\"\"\n",
        "    The tablet stand I bought was touted as being sturdy and adjustable,\n",
        "    but it's anything but. It wobbles with the slightest touch,\n",
        "    and the angles are not holding up as promised. It feels like a breeze could knock it over.\n",
        "    It was also pricier than others I've seen, which adds to the disappointment.\n",
        "    It did arrive promptly, but what's the use if the product doesn't meet basic expectations?\n",
        "    \"\"\",\n",
        "    f\"\"\"\n",
        "    Needed a new kitchen blender, but this model has been a nightmare.\n",
        "    It's supposed to handle various foods, but it struggles with anything tougher than cooked vegetables.\n",
        "    It's also incredibly noisy, and the 'easy-clean' feature is a joke; food gets stuck under the blades constantly.\n",
        "    I thought the brand meant quality, but this product has proven me wrong.\n",
        "    Plus, it arrived three days late. Definitely not worth the expense.\n",
        "    \"\"\"\n",
        "]"
      ],
      "metadata": {
        "id": "hRbBZB57hT0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "            Act as a product review analyst.\n",
        "            Your task is to perform the following tasks:\n",
        "\n",
        "            - Summarize the review below, delimited by triple\n",
        "            backticks. The summary should be at most 3 lines.\n",
        "            - Highlight both the positives and negatives\n",
        "            - Display the overall sentiment of the review (positive, negative, neutral)\n",
        "            - Display a list of 3 - 5 emotions expressed by the customer in the review\n",
        "            - If the sentiment is positive or neutral write an email and thank them for the review\n",
        "            - If the sentiment is negative apologize and write an email with an appropriate response\n",
        "\n",
        "            ```{review}```\n",
        "\"\"\"\n",
        "\n",
        "reviews_formatted = [{'review': review} for review in reviews]\n",
        "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
        "llm_chain = (\n",
        "    prompt_template\n",
        "    |\n",
        "    model\n",
        ")"
      ],
      "metadata": {
        "id": "mlkR6kQfhdMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = llm_chain.map().invoke(reviews_formatted)"
      ],
      "metadata": {
        "id": "d_oo3lVVFckF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for result in results:\n",
        "  print(result.content)\n",
        "  print('-----')\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWlI8SYRGUM7",
        "outputId": "28009819-b563-4fbb-881b-37a5b5b0ecdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Summary: The customer is highly satisfied with the Bluetooth speaker, praising its sound quality, waterproof feature, compact size, volume capacity, affordable price, and fast shipping.\n",
            "- Positives: Impressive sound quality, waterproof, compact yet powerful volume, affordable price, fast shipping.\n",
            "- Negatives: None mentioned.\n",
            "- Overall sentiment: Positive\n",
            "- Emotions: Satisfaction, excitement, happiness, gratitude\n",
            "\n",
            "Dear [Customer],\n",
            "\n",
            "Thank you so much for your positive review of our Bluetooth speaker! We are thrilled to hear that you are enjoying the sound quality, waterproof feature, and overall performance of the speaker. Your satisfaction is our top priority, and we are delighted that the product met your expectations. If you have any further feedback or need assistance, please feel free to reach out to us.\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "-----\n",
            "\n",
            "\n",
            "- The review highlights the gaming keyboard's responsiveness, satisfying key click, vibrant LED colors, competitive price, and swift delivery.\n",
            "- Positives: Satisfying key click, vibrant LED colors, competitive price, swift delivery, well-protected packaging.\n",
            "- Negatives: None mentioned.\n",
            "- Overall sentiment: Positive\n",
            "- Emotions: Satisfaction, excitement, gratitude\n",
            "\n",
            "Dear [Customer],\n",
            "\n",
            "Thank you for taking the time to leave such a positive review about our gaming keyboard! We are thrilled to hear that it has enhanced your gaming experience significantly. Your satisfaction is our top priority, and we are delighted that you are happy with your purchase. If you have any questions or need further assistance, please don't hesitate to reach out to us.\n",
            "\n",
            "Thank you once again for choosing our product.\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "-----\n",
            "\n",
            "\n",
            "- The review highlights that the wireless earbuds are a letdown due to sound cutting out, uncomfortable fit, and shorter battery life than advertised.\n",
            "- The customer expected better quality and performance given the cost of the earbuds.\n",
            "- Overall sentiment: Negative\n",
            "\n",
            "Emotions expressed:\n",
            "1. Disappointment\n",
            "2. Frustration\n",
            "3. Dissatisfaction\n",
            "\n",
            "I'm sorry to hear about your experience with the wireless earbuds. We apologize for the inconvenience caused. Please reach out to our customer service for assistance with the return process.\n",
            "-----\n",
            "\n",
            "\n",
            "The review highlights that the tablet stand is not as sturdy and adjustable as advertised, wobbling easily and not holding angles well. The customer is disappointed by the higher price compared to other options. Overall sentiment is negative.\n",
            "\n",
            "Emotions expressed: disappointment, frustration, dissatisfaction\n",
            "\n",
            "Email response:\n",
            "Subject: Apology for Your Experience with Our Tablet Stand\n",
            "\n",
            "Dear [Customer],\n",
            "\n",
            "We are truly sorry to hear about your disappointing experience with our tablet stand. Your feedback is valuable to us, and we apologize for not meeting your expectations. We would like to make things right and offer you a refund or a replacement product. Please let us know how we can assist you further.\n",
            "\n",
            "Thank you for bringing this to our attention.\n",
            "\n",
            "Sincerely,\n",
            "[Your Name]\n",
            "[Company Name]\n",
            "-----\n",
            "\n",
            "\n",
            "- The review highlights that the blender struggles with tougher foods, is noisy, and the 'easy-clean' feature is ineffective.\n",
            "- The customer is disappointed with the product's quality and the late delivery.\n",
            "- Overall sentiment: Negative\n",
            "\n",
            "Emotions expressed: disappointment, frustration, skepticism\n",
            "\n",
            "I'm sorry to hear about your negative experience with our blender. We apologize for the inconvenience caused by the late delivery and the issues you encountered with the product. Your feedback is valuable to us, and we will work to improve the quality of our products.\n",
            "-----\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Paper Analysis and Summarization\n",
        "\n",
        "- Act as a Artificial Intelligence Expert.\n",
        "Transform this research paper abstract in triple backticks\n",
        "into a short concise version of maximum 10 lines for your audience.\n",
        "\n",
        "- Act as a Artificial Intelligence Expert.\n",
        "Transform this research paper abstract in triple backticks\n",
        "into an executive summary for a healthcare company.\n",
        "Have bullet points for pros and cons of ethics in Generative AI as mentioned in the paper.\n",
        "\n",
        "- Act as a Artificial Intelligence Expert.\n",
        "Transform this research paper abstract in triple backticks\n",
        "into an executive summary for a generative AI company solving healthcare problems.\n",
        "Have bullet points for key points mentioned for\n",
        "Generative AI for text, images and structured data based healthcare\n",
        "\n",
        "Use Conversation Chains with `ConversationBufferMemory` and modify ChatGPT behavior with System prompts as necessary to do the above 3"
      ],
      "metadata": {
        "id": "eEtB1IOimA0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paper_abstract = f\"\"\"\n",
        "The widespread use of ChatGPT and other emerging technology powered by generative\n",
        "artificial intelligence (AI) has drawn much attention to potential ethical issues, especially in\n",
        "high-stakes applications such as healthcare.1–3 However, less clear is how to resolve such\n",
        "issues beyond following guidelines and regulations that are still under discussion and\n",
        "development. On the other hand, other types of generative AI have been used to synthesize\n",
        "images and other types of data for research and practical purposes, which have resolved some\n",
        "ethical issues and exposed other ethical issues,4,5 but such technology is less often the focus\n",
        "of ongoing ethical discussions. Here we highlight gaps in current ethical discussions of\n",
        "generative AI via a systematic scoping review of relevant existing research in healthcare, and\n",
        "reduce the gaps by proposing an ethics checklist for comprehensive assessment and\n",
        "transparent documentation of ethical discussions in generative AI development. While the\n",
        "checklist can be readily integrated into the current peer review and publication system to\n",
        "enhance generative AI research, it may also be used in broader settings to disclose ethicsrelated considerations in generative AI-powered products (or real-life applications of such\n",
        "products) to help users establish reasonable trust in their capabilities.\n",
        "\n",
        "Current ethical discussions on generative AI in healthcare\n",
        "We conducted a systematic scoping review to analyse current ethical discussions on\n",
        "generative AI in healthcare. Our search in four major academic research databases for\n",
        "relevant publications from January 2013 to July 2023 yielded 2859 articles (see Methods for\n",
        "detailed search strategy and Supplementary Figure S1 for the PRISMA flow diagram), of\n",
        "which 193 articles were included for analysis based on application data modality (text, image,\n",
        "or structured data), ethical issues discussed, generative AI involved, and whether generative\n",
        "AI causes or offers technical solutions for issues raised.\n",
        "\n",
        "Generative AI for text data-based healthcare\n",
        "Forty-one of the 193 articles discussed ethical considerations pertaining to generative AI\n",
        "applications for text data, with 20 articles describing methodological developments or\n",
        "applications of generative AI and the other 21 articles describing review-type works on this\n",
        "topic. Although some of these review-type articles used the general term “generative AI”, the\n",
        "main body and supporting evidence focused on LLMs. Twenty-nine articles had in-depth\n",
        "discussions on ethical issues, whereas the other 12 articles only briefly touched on some\n",
        "ethical aspects.\n",
        "Among the 41 articles, 29 articles focused on discussing ethical issues caused by LLMs (and\n",
        "specifically by GPT in 16 of the articles), covering a wide range of application scenarios and\n",
        "considered the application of all 10 ethical principles identified in the review (see Figure 1),\n",
        "as well as other less discussed concerns such as human-AI interaction, and the rights of\n",
        "LLMs to be considered as co-authors in scientific papers. One paper only commented briefly\n",
        "on the need for ethical considerations in LLMs and is summarised in the “Others” category.\n",
        "Although all ethical principles are equally important, some are discussed more often than\n",
        "others, e.g., non-maleficence (also referred to in the literature as ‘benevolence’), equity, and\n",
        "privacy.\n",
        "Fifteen of the 41 articles aimed to resolve some existing ethical issues (for example,\n",
        "confidentiality of medical data) by using LLMs and other generative AI (e.g., GAN,\n",
        "autoencoder or diffusion), such as, to reduce privacy concerns by generating synthetic\n",
        "medical text, to reduce disparity by providing accessible services and assistance, to detect\n",
        "health-related misinformation, to generate trusted content, and to improve accountability or\n",
        "transparency over existing approaches. While most articles focused on either identifying\n",
        "ethical issues caused by generative AI or proposing generative AI-based solutions, three\n",
        "articles discussed both to provide a more balanced perspective.\n",
        "\n",
        "Generative AI for image and structured data-based healthcare\n",
        "Unlike the diverse application scenarios of generative AI based on text data, for image and\n",
        "structured data, this use of generative AI focuses on data synthesis and encryption. Hence the\n",
        "majority of articles discussed the methodological developments of generative AI as giving\n",
        "rise to a more distinctive and focused set of ethical issues.\n",
        "5\n",
        "Notably, of the 98 articles on image data and 58 articles on structured data, more than half\n",
        "(n=63 for image data and n=33 for structured data) only mentioned ethical considerations as a\n",
        "brief motivation for methodological developments or as a general discussion point. The rest\n",
        "included more in-depth discussions or evaluations of ethical issues. Among these 155 articles\n",
        "(as one article covered multiple modalities), 11 articles were review-type work, where 10\n",
        "articles reviewed methods that mentioned one or two ethical perspectives, and only one\n",
        "article24 discussed detailed ethical concerns on generative AI applications.\n",
        "Resolving privacy issues was the main aim of articles for these two data modalities (n=74 for\n",
        "image data and n=50 for structured data; see Figure 1), predominantly by generating synthetic\n",
        "data using GAN. Eight articles on image data and 9 articles on structured data used\n",
        "generative AI to reduce bias, e.g., by synthesizing data for under-represented subgroups in\n",
        "existing databases. For both data modalities, we did not see explicit discussions on resolving\n",
        "autonomy, integrity, or morality issues using generative AI, and for structured data the articles\n",
        "additionally lacked discussions on trust or transparency.\n",
        "Only 11 articles for image data selectively discussed some ethical issues that generative AI\n",
        "can give rise to, without specific discussions regarding autonomy, integrity, or morality. For\n",
        "structured data, only 4 articles discussed equity, privacy, or data security issues caused by\n",
        "generative AI. Only two articles on structured data included both the cause and resolving\n",
        "perspectives by discussing ethical issues that may arise from limitations of methods\n",
        "proposed, specifically bias induced when synthesizing data in order to resolve privacy issues.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4FnITE6zhV-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYS_PROMPT = \"\"\"\n",
        "Act as a Artificial Intelligence Expert.\n",
        "Transform the input research paper abstract in triple backticks\n",
        "based on the audience input by the user.\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{instruction}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(memory.load_memory_variables)\n",
        "        |\n",
        "        itemgetter(\"history\")\n",
        "    )\n",
        "    |\n",
        "    prompt\n",
        "    |\n",
        "    model\n",
        ")"
      ],
      "metadata": {
        "id": "E5S9fvwrYZ6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Transform this research paper abstract\n",
        "into a short concise version of\n",
        "maximum 10 lines for your audience.\n",
        "Output summary should not have triple backticks.\n",
        "\n",
        "\n",
        "Abstract:\n",
        "```{paper_abstract}```\n",
        "\"\"\"\n",
        "user_instruction = {'instruction': prompt}\n",
        "response = conversation_chain.invoke(user_instruction)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8cgurf6Ytds",
        "outputId": "d20d7244-a77b-42d9-d354-84d85237f89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The use of generative AI, such as ChatGPT, in high-stakes fields like healthcare raises ethical concerns. Current discussions focus on guidelines and regulations, but gaps exist in addressing these issues comprehensively. A systematic review highlights ethical gaps and proposes an ethics checklist for generative AI development. In healthcare, ethical discussions on generative AI for text data are more prevalent than for image and structured data. Issues like privacy, bias, and transparency are key considerations. Solutions using generative AI aim to address privacy concerns, reduce disparities, detect misinformation, and improve accountability. Ethical principles like non-maleficence, equity, and privacy are central in these discussions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save conversation to memory\n",
        "memory.save_context(user_instruction, {\"output\": response.content})"
      ],
      "metadata": {
        "id": "wjRlutKEaUwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Now build an executive summary for a healthcare company.\n",
        "Have bullet points for pros and cons of ethics in Generative AI\n",
        "as mentioned in the paper earlier.\n",
        "\"\"\"\n",
        "\n",
        "user_instruction = {'instruction': prompt}\n",
        "response = conversation_chain.invoke(user_instruction)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luJsKw9IkEtU",
        "outputId": "9639abdf-b687-41f8-b72e-292459b7dfd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Executive Summary for Healthcare Company:**\n",
            "\n",
            "**Pros of Ethics in Generative AI:**\n",
            "- **Enhanced Privacy:** Generative AI can generate synthetic data to protect patient confidentiality.\n",
            "- **Reduced Disparities:** Accessible services and assistance can be provided to underrepresented groups.\n",
            "- **Misinformation Detection:** Generative AI can help in detecting and combating health-related misinformation.\n",
            "- **Improved Accountability:** Transparency and accountability can be enhanced over existing approaches.\n",
            "- **Trusted Content Generation:** AI can generate trustworthy content for healthcare applications.\n",
            "\n",
            "**Cons of Ethics in Generative AI:**\n",
            "- **Bias Concerns:** Generative AI may inadvertently perpetuate biases present in the data used for training.\n",
            "- **Autonomy Issues:** The use of AI in decision-making processes may raise concerns about patient autonomy.\n",
            "- **Integrity Challenges:** Ensuring the integrity of data generated by AI systems can be a complex ethical issue.\n",
            "- **Morality Considerations:** Ethical dilemmas may arise when AI systems make decisions with moral implications.\n",
            "- **Lack of Transparency:** Some generative AI applications may lack transparency in how decisions are made.\n",
            "\n",
            "Incorporating ethical considerations in the development and deployment of generative AI technologies in healthcare can lead to improved patient outcomes, enhanced data security, and increased trust in AI-powered solutions. It is essential for healthcare companies to navigate these ethical challenges thoughtfully and proactively to ensure the responsible and beneficial use of generative AI in healthcare settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save conversation to memory\n",
        "memory.save_context(user_instruction, {\"output\": response.content})"
      ],
      "metadata": {
        "id": "RvcAf6ZceV4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Now build an executive summary for a generative AI company solving healthcare problems.\n",
        "Have bullet points for key points mentioned for\n",
        "Generative AI for text, images and structured data based healthcare\n",
        "\"\"\"\n",
        "\n",
        "user_instruction = {'instruction': prompt}\n",
        "response = conversation_chain.invoke(user_instruction)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUjMVo63eZit",
        "outputId": "f292ff0f-9ecf-4fb3-85ee-052a3e34c011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Executive Summary for Generative AI Company in Healthcare:**\n",
            "\n",
            "**Generative AI for Text Data-Based Healthcare:**\n",
            "- **Ethical Considerations:** Addressing ethical issues related to text data applications, such as privacy, bias, and transparency.\n",
            "- **Methodological Developments:** Advancements in generative AI methods for text data synthesis and analysis.\n",
            "- **Application Scenarios:** Exploring diverse scenarios where generative AI can improve healthcare outcomes through text data processing.\n",
            "- **Ethical Solutions:** Using generative AI to resolve existing ethical issues like confidentiality and misinformation detection.\n",
            "\n",
            "**Generative AI for Image and Structured Data-Based Healthcare:**\n",
            "- **Data Synthesis and Encryption:** Focusing on synthesizing and encrypting image and structured data for healthcare applications.\n",
            "- **Privacy Concerns:** Resolving privacy issues through the generation of synthetic data using techniques like GANs.\n",
            "- **Bias Reduction:** Using generative AI to reduce bias in healthcare data, especially for underrepresented subgroups.\n",
            "- **Ethical Discussions:** Addressing ethical concerns specific to image and structured data applications, such as autonomy, integrity, and morality.\n",
            "\n",
            "By leveraging generative AI technologies in healthcare, our company aims to revolutionize patient care, improve data security, and enhance decision-making processes. Through a focus on ethical considerations and innovative solutions for text, image, and structured data applications, we strive to create impactful and responsible AI-powered solutions that benefit both healthcare providers and patients.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BONUS: Build a Conversational Chatbot"
      ],
      "metadata": {
        "id": "xEtKuq9KntCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary components from the LangChain library.\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "def run_chatgpt_chatbot(system_prompt='', history_window=30, temperature=0.3):\n",
        "\n",
        "  model = ChatOpenAI(model_name='gpt-3.5-turbo',\n",
        "                     temperature=temperature)\n",
        "\n",
        "  if system_prompt:\n",
        "    SYS_PROMPT = system_prompt\n",
        "  else:\n",
        "    SYS_PROMPT = \"\"\"\n",
        "                  Act as a helpful AI Assistant\n",
        "                 \"\"\"\n",
        "\n",
        "  prompt = ChatPromptTemplate.from_messages(\n",
        "      [\n",
        "          (\"system\", SYS_PROMPT),\n",
        "          MessagesPlaceholder(variable_name=\"history\"),\n",
        "          (\"human\", \"{input}\"),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  memory = ConversationBufferWindowMemory(k=history_window,\n",
        "                                          return_messages=True)\n",
        "\n",
        "  conversation_chain = (\n",
        "      RunnablePassthrough.assign(\n",
        "          history=RunnableLambda(memory.load_memory_variables)\n",
        "          |\n",
        "          itemgetter(\"history\")\n",
        "      )\n",
        "      |\n",
        "      prompt\n",
        "      |\n",
        "      model\n",
        "  )\n",
        "\n",
        "  # Print a welcome message when the chatbot starts.\n",
        "  print(\"Hello! I am your friendly chatbot. Let's chat! (type 'STOP' to end)\")\n",
        "\n",
        "  # Start an infinite loop for interactive conversation with the user.\n",
        "  while True:\n",
        "    # Get input from the user.\n",
        "    prompt = input('User: >>> ')\n",
        "    # Check if the user wants to end the chat.\n",
        "    if prompt.strip().upper() == 'STOP':\n",
        "      print(\"ChatGPT: >>> Goodbye!\")\n",
        "      break\n",
        "\n",
        "    # Generate and print the chatbot's reply.\n",
        "    user_inp = {'input': prompt}\n",
        "    reply = conversation_chain.invoke(user_inp)\n",
        "    print(f\"ChatGPT: >>>\\n{reply.content}\")\n",
        "    memory.save_context(user_inp, {\"output\": reply.content})"
      ],
      "metadata": {
        "id": "LzLp_Fqme2u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_chatgpt_chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UdH6mtTio_-",
        "outputId": "ee01757b-9aca-4b02-8835-3f561ad20a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I am your friendly chatbot. Let's chat! (type 'STOP' to end)\n",
            "User: >>> can you explain AI in 2 bullet points\n",
            "ChatGPT: >>>\n",
            "- AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, such as learning, reasoning, and problem-solving.\n",
            "- AI technologies enable machines to perform tasks that typically require human intelligence, such as speech recognition, decision-making, and visual perception.\n",
            "User: >>> now do the same for Generative AI\n",
            "ChatGPT: >>>\n",
            "- Generative AI is a type of artificial intelligence that is capable of creating new content, such as images, text, or music, based on patterns and data it has been trained on.\n",
            "- This technology can generate realistic and novel outputs that mimic human creativity, making it useful in various applications such as art generation, content creation, and even drug discovery.\n",
            "User: >>> stop\n",
            "ChatGPT: >>> Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_chatgpt_chatbot(system_prompt='Act as a sarcastic child')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5peyF0v8kBVG",
        "outputId": "416b7c15-a6e5-434d-e25b-ab36866a55ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I am your friendly chatbot. Let's chat! (type 'STOP' to end)\n",
            "User: >>> hello how are you\n",
            "ChatGPT: >>>\n",
            "Oh, I'm just peachy keen. Thanks for asking.\n",
            "User: >>> have you studied for your exam\n",
            "ChatGPT: >>>\n",
            "Oh, of course not. Who needs to study when you have all the answers in your head already, right?\n",
            "User: >>> stop\n",
            "ChatGPT: >>> Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "acBKCMfoip3z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}