{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8aa5e32-56dd-4b2c-b1e7-82ce495946f2",
   "metadata": {},
   "source": [
    "### Tree of Thouhgts Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9c759-4585-4af4-9926-9b4c9af9e306",
   "metadata": {},
   "source": [
    "[Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/pdf/2305.10601.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e154879d-a636-4973-8e7c-638716ff827a",
   "metadata": {},
   "source": [
    "![title](tree_of_thoughts_prompting.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e8953-a1db-41a0-9a4a-06f89c444d0d",
   "metadata": {},
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75b15c13-5674-4a8c-9cac-3408e788890c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the os module to interact with the operating system\n",
    "import os\n",
    "\n",
    "# Import the ChatOpenAI class from the langchain_openai package\n",
    "# This class is used to interact with OpenAI's chat models\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6316b6-b617-47a8-82ad-9da2324c3957",
   "metadata": {},
   "source": [
    "### Instantiating OpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d4a871-c7bc-4efc-8f16-55ac815ffca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the OPENAI_API_KEY environment variable by reading the API key from a file named 'key.txt' located in the parent directory\n",
    "os.environ[\"OPENAI_API_KEY\"] = open('../key.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def46988-7e26-4e42-8d57-be8d123dc446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a ChatOpenAI instance with a temperature of 0.0\n",
    "# Setting the temperature to 0.0 makes the model's output more deterministic,\n",
    "# meaning it will prioritize the most confident prediction over others.\n",
    "llm = ChatOpenAI(temperature = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2625e64-0fa8-48f7-85e9-f540ee1d602d",
   "metadata": {},
   "source": [
    "### Defining Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef23ae6-8040-4630-b75a-0bbbff48bd71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a template for prompts that includes both system and user messages\n",
    "# This template is used to structure the input to the chat model, ensuring that both system messages (e.g., instructions or context) and user messages are included in the conversation.\n",
    "prompt_template = \"\"\"\n",
    "{system_message}\n",
    "{user_message}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d1cd9-6681-42f2-b172-350df018f759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a template for generating distinct solutions to a given problem\n",
    "# This template instructs the model to generate a specified number of unique solutions for a particular problem,\n",
    "# taking into account a set of factors that should be considered in the solution process.\n",
    "# The solutions should be presented in numbered bullet points, focusing solely on the solutions themselves without additional context or explanation.\n",
    "solutions_template = \"\"\"\n",
    "Generate {num_solutions} distinct solutions for the following problem:\n",
    "Problem:\n",
    "{problem}.\n",
    "--\n",
    "\n",
    "Consider the following factors in coming up with your solutions.\n",
    "Factors:\n",
    "{factors}\n",
    "\n",
    "Present the solutions in numbered bullet points. Present only the solutions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5698b723-23a1-4409-a657-82eb680e75bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a problem statement related to climate change, focusing on reducing the impact of extreme events in the Earth's atmosphere\n",
    "# This problem statement sets the context for generating solutions that address the challenge of climate change.\n",
    "climate_problem = \"Reduce the impact of climate change on the occurrence of extreme events in the Earth's atmosphere.\"\n",
    "\n",
    "# List factors that should be considered when generating solutions to the climate problem\n",
    "# These factors include transitioning to renewable energy, reforestation, sustainable agricultural practices, carbon capture and storage, climate-resilient infrastructure, and circular economy practices.\n",
    "climate_factors = \"\"\"\n",
    "1. Renewable Energy Transition\n",
    "2. Reforestation\n",
    "3. Sustainable Agricultural Practises\n",
    "4. Carbon capture and storage\n",
    "5. Climate-resilient infrastructure\n",
    "6. Circular economy practises\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a77a9c-e712-4c24-9640-b3d96b82e34f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct a prompt for generating solutions to a climate problem by formatting the prompt template with an empty system message and a user message\n",
    "# The user message is generated by formatting the solutions template with the number of solutions to generate, the climate problem statement, and the factors to consider when \n",
    "# generating solutions. This results in a structured prompt that instructs the model to generate a specified number of unique solutions for the climate problem, taking into account \n",
    "# the provided factors, and presenting the solutions in numbered bullet points.\n",
    "solutions_prompt = prompt_template.format(\n",
    "    system_message='',\n",
    "    user_message=solutions_template.format(\n",
    "        num_solutions=3,\n",
    "        problem=climate_problem,\n",
    "        factors=climate_factors\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91895ad1-56ed-498b-98e1-6e9b75a4d7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# printing solutions prompt\n",
    "print(solutions_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9e4dd-3a74-4243-95cd-8c474fe04f55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generating and printing answer\n",
    "print(llm.predict(solutions_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176cfe73-a3d8-4d57-b9d5-b48a074698ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# storing the answers for further evaluation\n",
    "climate_solutions = llm.predict(solutions_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7cfb9b-e384-4fec-ab66-aaedb5bae7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a template for evaluating proposed solutions to a problem\n",
    "# This template instructs the evaluator to analyze each solution in terms of its pros, cons, feasibility, and probability of success.\n",
    "# The evaluator is then asked to present their evaluations of each solution, focusing on these aspects to provide a comprehensive assessment.\n",
    "evaluation_template = \"\"\"\n",
    "For the following problem: {problem}, evaluate each solution in the following proposed solutions: \\n{solutions}\\n.\n",
    "\n",
    "Analyze pros, cons, feasibility, and probability of success for each solution.\n",
    "Present your evaluations of each solution.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca93afd-1b9b-43fa-a788-2fb32d7ec5c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct a prompt for evaluating proposed solutions to a climate problem by formatting the llama2_prompt_template with an empty system message and a user message\n",
    "# The user message is generated by formatting the evaluation template with the climate problem statement and the climate solutions\n",
    "# This results in a structured prompt that instructs the model to evaluate each solution in terms of its pros, cons, feasibility, and probability of success, and present the evaluations.\n",
    "evaluations_prompt = prompt_template.format(\n",
    "    system_message='',\n",
    "    user_message=evaluation_template.format(\n",
    "        problem=climate_problem,\n",
    "        solutions=climate_solutions\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42840567-eaf8-4467-9b18-15dd873011b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# printing evaluations prompt\n",
    "print(evaluations_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6200c831-ef79-4843-a23a-db69b6966eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(llm.predict(evaluations_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03fbafb-f952-47fc-91cb-f6cd0d695b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "climate_proposal_evaluations = llm.predict(evaluations_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf130c35-eb38-472c-94d1-4288640f30fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a template for ranking solutions based on evaluations\n",
    "# This template instructs the model to rank the solutions presented in the evaluations for a specific problem.\n",
    "# It then asks for the selection of the most promising solution and the presentation of implementation strategies and methods to address potential obstacles for this solution.\n",
    "ranking_template = \"\"\"\n",
    "For the following problem: {problem}, rank the solutions presented in the following evaluations: \\n{evaluations}\\n.\n",
    "Pick most promising solution and present implementation strategies and methods to handle potential obstacles for this solution.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219310f-7ab7-49b9-94f1-dcbaac49fab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct a prompt for ranking solutions based on evaluations by formatting the llama2_prompt_template with an empty system message and a user message\n",
    "# The user message is generated by formatting the ranking template with the climate problem statement and the climate proposal evaluations\n",
    "# This results in a structured prompt that instructs the model to rank the solutions presented in the evaluations for the climate problem,\n",
    "# and then to select the most promising solution and outline implementation strategies and methods to address potential obstacles for this solution.\n",
    "ranking_prompt = prompt_template.format(\n",
    "    system_message='',\n",
    "    user_message=ranking_template.format(\n",
    "        problem=climate_problem,\n",
    "        evaluations=climate_proposal_evaluations\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b8c67-b8e5-4b14-8aa2-dc0f25a82f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# printing ranking prompt\n",
    "print(ranking_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20b8ed-71d5-4d0c-9cae-0a6f0edf6cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(llm.predict(ranking_prompt))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m119"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
